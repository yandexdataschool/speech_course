{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture #11 Seminar \"WaveNet inference\" üåä\n",
    "\n",
    "\n",
    "The purpose of this seminar is to immerse you in WaveNet architecture. In the lecture we discussed in detail WaveNet architecture, so you can easily write an autoregressive inference function.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Mark-Hasegawa-Johnson/publication/311106829/figure/fig3/AS:433958858039299@1480475262377/The-caching-scheme-for-efficient-generation-Due-to-dilated-convolutions-the-size-of-the.png\" alt=\"Drawing\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.signal import lfilter\n",
    "from queue import Queue\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSP utils \n",
    "\n",
    "\"D\" in \"Deep Learning\" stands for data and \"e\" for \"engineering\" üôÉ, so we need to implement some basic data preprocessing functions. Specifically we need to implement proper conversion from signal to mel spectrogram.\n",
    "\n",
    "All preprocessing functions are already implemented, but we don't want you to pass them by üòè. So you need to implement their reverse counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    MAX_WAV_VALUE = 32768.0\n",
    "\n",
    "    def __init__(self):\n",
    "        num_frequencies = 1025\n",
    "        self.sample_rate = 24000\n",
    "        self.window_size = int(self.sample_rate * 0.05)\n",
    "        self.window_step = self.window_size // 4\n",
    "        self.n_fft = (num_frequencies - 1) * 2\n",
    "        self.preemphasis_coef = 0.97\n",
    "        self.min_frequency = 50\n",
    "        self.max_frequency = 12000\n",
    "        self.num_mel_bins = 80\n",
    "        self.ref_level_db = 20\n",
    "        self.min_level_db = -100\n",
    "        \n",
    "        self.min_level = np.exp(self.min_level_db / 20 * np.log(10))\n",
    "\n",
    "        self.mel_basis = librosa.filters.mel(\n",
    "            self.sample_rate,\n",
    "            n_fft=self.n_fft,\n",
    "            n_mels=self.num_mel_bins,\n",
    "            fmin=self.min_frequency,\n",
    "            fmax=self.max_frequency)\n",
    "        self.inv_mel_basis = np.linalg.pinv(self.mel_basis)\n",
    "        \n",
    "    def load_wav(self, path):\n",
    "        sr, signal = read(path)\n",
    "        if signal.dtype == np.int16:\n",
    "            signal = signal.astype(np.float32) / self.MAX_WAV_VALUE\n",
    "        assert sr == self.sample_rate\n",
    "        return signal\n",
    "\n",
    "    def stft(self, y):\n",
    "        return librosa.stft(y,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.window_step,\n",
    "            win_length=self.window_size)\n",
    "\n",
    "    def istft(self, y):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def pre_emphasis(self, x):\n",
    "        return lfilter([1, -self.preemphasis_coef], [1], x)\n",
    "\n",
    "    def de_emphasis(self, x):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def amp_to_db(self, x):\n",
    "        x = np.maximum(self.min_level, x)\n",
    "        return 20 * np.log10(x) - self.ref_level_db\n",
    "\n",
    "    def db_to_amp(self, x):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def normalize(self, S):\n",
    "        return np.clip((S - self.min_level_db) / -self.min_level_db, 0, 1)\n",
    "\n",
    "    def inv_normalize(self, S):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def linear_to_mel(self, S):\n",
    "        return np.dot(self.mel_basis, S)\n",
    "\n",
    "    def mel_to_linear(self, M):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def spectrogram(self, y):\n",
    "        S = np.abs(self.stft(y))\n",
    "        S = self.amp_to_db(S)\n",
    "        return self.normalize(S)\n",
    "\n",
    "    def inv_spectrogram(self, S):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "    \n",
    "    def mel_spectrogram(self, y):\n",
    "        S = np.abs(self.stft(y))\n",
    "        M = self.linear_to_mel(S)\n",
    "        M = self.amp_to_db(M)\n",
    "        return self.normalize(M)\n",
    "\n",
    "    def inv_mel_spectrogram(self, M):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def griffin_lim(self, x, num_iter=20):\n",
    "        # not the first time, I know :)\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def mu_law_encode(self, x, mu=256):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def mu_law_decode(self, x, mu=256):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "    \n",
    "ap = AudioProcessor()\n",
    "\n",
    "x = ap.load_wav('./samples/00000.wav')\n",
    "spec = ap.spectrogram(ap.pre_emphasis(x))\n",
    "mel = ap.mel_spectrogram(ap.pre_emphasis(x))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(spec, aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "plt.imshow(mel, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all tests are passed\n",
    "\n",
    "x = ap.load_wav('./samples/00000.wav')\n",
    "\n",
    "assert np.abs(ap.de_emphasis(ap.pre_emphasis(x)) - x).max() < 1e-10\n",
    "\n",
    "assert ap.mu_law_encode(-1.0) == 0\n",
    "assert ap.mu_law_encode(0.0) == 128\n",
    "assert ap.mu_law_encode(0.5) == 239\n",
    "assert ap.mu_law_encode(1.0) == 255\n",
    "for i in range(256):\n",
    "    assert ap.mu_law_encode(ap.mu_law_decode(i)) == i\n",
    "\n",
    "# make sure the quality does not degrade too much\n",
    "display(Audio(x, rate=ap.sample_rate))\n",
    "display(Audio(ap.inv_spectrogram(ap.spectrogram(x)), rate=ap.sample_rate))\n",
    "display(Audio(ap.inv_mel_spectrogram(ap.mel_spectrogram(x)), rate=ap.sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Griffin-Lim with the synthesised üåÆ mels\n",
    "\n",
    "Hear how the result of Griffin-Lim sounds, not just on mel spectrograms, but on **generated** mel spectrograms. But it was generated using teacher forcing, otherwise it will not be possible to accurately correlate the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ap.load_wav('./samples/00000.wav')\n",
    "m = np.load('./samples/00000.npy')\n",
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "plt.imshow(m.T, aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "display(Audio(x, rate=ap.sample_rate))\n",
    "display(Audio(ap.de_emphasis(ap.inv_mel_spectrogram(m.T)), rate=ap.sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveNet\n",
    "\n",
    "Read the code carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CausalConv1d, self).__init__(*args, **kwargs)\n",
    "        self.padding = ((self.kernel_size[0] - 1) * self.dilation[0],)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super(CausalConv1d, self).forward(x)\n",
    "        return x[:, :, :-self.padding[0]]\n",
    "\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    \"\"\"WaveNet architecture with local conditioning\n",
    "\n",
    "    https://arxiv.org/pdf/1609.03499.pdf - original paper\n",
    "    https://arxiv.org/pdf/1702.07825.pdf - appending A for more details\n",
    "    \n",
    "    But given implementation has following differences:\n",
    "    1. tanh is not applied to input embedding\n",
    "    2. vector is scaled (multiplied 0.5 ** 0.5) between blocks\n",
    "    3. GRU is used for processing mel spectrogram\n",
    "    4. GRU output is nearest neighbour apsampled hop_size times\n",
    "    5. each block has own conditioning projection\n",
    "\n",
    "    Args:\n",
    "        num_channels       (int): size of modelled categorical distribution\n",
    "        residual_channels  (int): hidden vector size\n",
    "        gate_channels      (int): gate block dimension\n",
    "        skip_channels      (int): skip-vector size\n",
    "        pre_channels       (int): dimension before the last layer\n",
    "        dilation_cycles    (int): number of dilation cycles\n",
    "        dilation_depth     (int): blocks number in dilation cycle\n",
    "        condition_channels (int): number of mel filters\n",
    "        hop_size           (int): STFT hop size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_channels,\n",
    "                 residual_channels,\n",
    "                 gate_channels,\n",
    "                 skip_channels,\n",
    "                 pre_channels,\n",
    "                 dilation_cycles,\n",
    "                 dilation_depth,\n",
    "                 condition_channels,\n",
    "                 hop_size):\n",
    "        super(WaveNet, self).__init__()\n",
    "        \n",
    "        self.kernel_size = 2\n",
    "        self.dilations = np.array([\n",
    "            2 ** (i % dilation_depth) \n",
    "            for i in range(dilation_cycles * dilation_depth)\n",
    "        ])\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.gate_channels = gate_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.pre_channels = pre_channels\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        self.condition_net = nn.GRU(\n",
    "            input_size=condition_channels,\n",
    "            hidden_size=condition_channels // 2,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "        \n",
    "        self.conv_input = nn.Conv1d(\n",
    "            in_channels=num_channels,\n",
    "            out_channels=residual_channels,\n",
    "            kernel_size=1)\n",
    "\n",
    "        self.blocks_conv_filter = nn.ModuleList([\n",
    "            CausalConv1d(\n",
    "                in_channels=residual_channels,\n",
    "                out_channels=gate_channels,\n",
    "                kernel_size=2,\n",
    "                dilation=d\n",
    "            ) for d in self.dilations])\n",
    "\n",
    "        self.blocks_conv_gate = nn.ModuleList([\n",
    "            CausalConv1d(\n",
    "                in_channels=residual_channels,\n",
    "                out_channels=gate_channels,\n",
    "                kernel_size=2,\n",
    "                dilation=d\n",
    "            ) for d in self.dilations])\n",
    "        \n",
    "        self.blocks_conv_residual = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=gate_channels,\n",
    "                out_channels=residual_channels,\n",
    "                kernel_size=1\n",
    "            ) for _ in range(len(self.dilations) - 1)])\n",
    "        \n",
    "        self.blocks_conv_skip = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=gate_channels,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=1\n",
    "            ) for _ in range(len(self.dilations))])\n",
    "        \n",
    "        self.blocks_conv_cond = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=condition_channels,\n",
    "                out_channels=gate_channels * 2,\n",
    "                kernel_size=1\n",
    "            ) for _ in range(len(self.dilations))])\n",
    "        \n",
    "        self.conv_out_1 = nn.Conv1d(\n",
    "            in_channels=skip_channels,\n",
    "            out_channels=pre_channels,\n",
    "            kernel_size=1)\n",
    "        self.conv_out_2 = nn.Conv1d(\n",
    "            in_channels=pre_channels,\n",
    "            out_channels=num_channels,\n",
    "            kernel_size=1)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (FloatTensor): continuous audio signal [B x K x T]\n",
    "            c (FloatTensor): local condition features [B x L x C],\n",
    "                where L = T // 300\n",
    "\n",
    "        Returns:\n",
    "            FloatTensor: output [B x out_channels x T]\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.conv_input(x)\n",
    "        c, _ = self.condition_net(c)\n",
    "        c = c.transpose(1, 2)\n",
    "\n",
    "        c = nn.functional.interpolate(c,\n",
    "            scale_factor=self.hop_size,\n",
    "            mode='nearest')\n",
    "\n",
    "        assert c.size(2) == x.size(2)\n",
    "\n",
    "        x_acc = 0\n",
    "        for b in range(len(self.dilations)):\n",
    "            x_filter = self.blocks_conv_filter[b](x)\n",
    "            x_gate = self.blocks_conv_gate[b](x)\n",
    "\n",
    "            cond = self.blocks_conv_cond[b](c)\n",
    "            c_filter, c_gate = cond.chunk(chunks=2, dim=1)\n",
    "            x_filter += c_filter\n",
    "            x_gate += c_gate\n",
    "\n",
    "            x_hidden = torch.tanh(x_filter) * torch.sigmoid(x_gate)\n",
    "\n",
    "            x_skip = self.blocks_conv_skip[b](x_hidden)\n",
    "            x_acc = x_acc + x_skip\n",
    "\n",
    "            if b < len(self.dilations) - 1:\n",
    "                x_residual = self.blocks_conv_residual[b](x_hidden)\n",
    "                x = x + x_residual\n",
    "\n",
    "            x = x * 0.5 ** 0.5\n",
    "            \n",
    "\n",
    "        x = self.conv_out_1(torch.relu(x_acc))\n",
    "        x = self.conv_out_2(torch.relu(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveNet(\n",
    "    num_channels=256,\n",
    "    residual_channels=64,\n",
    "    gate_channels=64,\n",
    "    skip_channels=128,\n",
    "    pre_channels=256,\n",
    "    dilation_cycles=4,\n",
    "    dilation_depth=10,\n",
    "    condition_channels=80,\n",
    "    hop_size=300)\n",
    "model.load_state_dict(torch.load('./state_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, num_channels):\n",
    "    x_o = torch.FloatTensor(x.shape[0], num_channels, x.shape[1])\n",
    "    x_o.zero_().scatter_(1, x.unsqueeze(1), 1)\n",
    "    return x_o\n",
    "\n",
    "def calc_loss(model, x, c):\n",
    "    x_o = one_hot(x, model.num_channels)\n",
    "    y = model.forward(x_o, c).transpose(1, 2)\n",
    "\n",
    "    loss = nn.functional.cross_entropy(\n",
    "        y[:, :-1].contiguous().view(-1, y.shape[-1]),\n",
    "        x[:, 1:].contiguous().view(-1))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = ap.load_wav('./samples/00000.wav')\n",
    "    c = np.load('./samples/00000.npy')\n",
    "    \n",
    "    # cut off to be a multiple of the window step\n",
    "    c = c[:len(x) // ap.window_step]\n",
    "    x = x[:len(c) * ap.window_step]\n",
    "\n",
    "    # apply mu-law encoding\n",
    "    x = ap.mu_law_encode(x)\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    c = torch.FloatTensor(c)\n",
    "\n",
    "    loss = calc_loss(model, x.unsqueeze(0), c.unsqueeze(0)).item()\n",
    "\n",
    "assert np.allclose(loss, 1.7863293886184692)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet Inference \n",
    "\n",
    "You need to implement the `infer` function, which synthesizes the audio from the `mel` spectrogram by the `model` model. The output of this function is mu-law encoded signal.\n",
    "\n",
    "**Important note**: as you can see from the code, we calculate spectrograms passing them through the pre-emphasis filter. It so happened (obviously by mistake) that although WaveNet learned with such spectrograms, but the audio signal for was not passed through the pre-emphasis filter. So you **do not need** to pass WaveNet output through de-emphassis filter.\n",
    "\n",
    "Hints:\n",
    "1. debug on short spectra (30-40 frames long)\n",
    "2. parse network to get matrices and vectors -- it's easier to work directly with them\n",
    "3. sanity check matrices sizes that they have expected shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, mel):\n",
    "    #####\n",
    "    # ...\n",
    "    #####\n",
    "    return np.array([0])\n",
    "    \n",
    "\n",
    "\n",
    "x = ap.load_wav('./samples/00000.wav')\n",
    "x_gen = ap.mu_law_decode(infer(model, np.load('./samples/00000.npy')))\n",
    "\n",
    "display(Audio(x, rate=ap.sample_rate))\n",
    "display(Audio(x_gen, rate=ap.sample_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
