{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chemical-cursor",
   "metadata": {},
   "source": [
    "# Seminar for Lecture 13 \"VAE Vocoder\"\n",
    "\n",
    "\n",
    "In the lectures, we studied various approaches to creating vocoders. The problem of sound generation is solved by deep generative models. We've discussed autoregressive models that can be reduced to **MAF**. We've considered the reverse analogue of MAF ‚Äì **IAF**. We've seen how **normalizing flows** can help us directly optimize likelihood without using autoregression. And alse we've considered a vocoder built with the **GAN** paradigm.\n",
    "\n",
    "At this seminar we will try to apply another popular generative model: the **variational autoencoder (VAE)**. We will try to build an encoder-decoder architecture with **MAF** as encoder and **IAF** as decoder. We will train this network by maximizing ELBO with a couple of additional losses (in vocoders, you can't do without it yet ü§∑‚Äç‚ôÇÔ∏è).\n",
    "\n",
    "‚ö†Ô∏è In this seminar we call **\"MAF\"** not the generative model discussed on lecture, but network which architecture is like MAF's one and accepting audio as input. So we won't model data distribution with our **\"MAF\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2  -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# ! pip install numpy==1.17.5 matplotlib==3.3.3 tqdm==4.54.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Union\n",
    "from math import log, pi, sqrt\n",
    "from IPython.display import display, Audio\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU found! üéâ')\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-fifty",
   "metadata": {},
   "source": [
    "Introduce auxiliary modules:\n",
    "1. causal convolution ‚Äì simple convolution with `kernel_size` and `dilation` hyper-parameters, but working in causal way (does not look in the future)\n",
    "2. residual block ‚Äì main building component of WaveNet architecture\n",
    "\n",
    "Yes, WaveNet is everywhere. We can build MAF and IAF with any architecture, but WaveNet declared oneself as simple yet powerfull architecture. We will use WaveNet with conditioning on mel spectrograms, because we are building a vocoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super(CausalConv, self).__init__()\n",
    "\n",
    "        self.padding = dilation * (kernel_size - 1)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=self.padding,\n",
    "            dilation=dilation)\n",
    "        self.conv = nn.utils.weight_norm(self.conv)\n",
    "        nn.init.kaiming_normal_(self.conv.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x[:, :, :-self.padding]\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_channels, kernel_size, dilation, cin_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.cin_channels = cin_channels\n",
    "\n",
    "        self.filter_conv = CausalConv(in_channels, out_channels, kernel_size, dilation)\n",
    "        self.gate_conv = CausalConv(in_channels, out_channels, kernel_size, dilation)\n",
    "        self.res_conv = nn.Conv1d(out_channels, in_channels, kernel_size=1)\n",
    "        self.skip_conv = nn.Conv1d(out_channels, skip_channels, kernel_size=1)\n",
    "        self.res_conv = nn.utils.weight_norm(self.res_conv)\n",
    "        self.skip_conv = nn.utils.weight_norm(self.skip_conv)\n",
    "        nn.init.kaiming_normal_(self.res_conv.weight)\n",
    "        nn.init.kaiming_normal_(self.skip_conv.weight)\n",
    "\n",
    "        self.filter_conv_c = nn.Conv1d(cin_channels, out_channels, kernel_size=1)\n",
    "        self.gate_conv_c = nn.Conv1d(cin_channels, out_channels, kernel_size=1)\n",
    "        self.filter_conv_c = nn.utils.weight_norm(self.filter_conv_c)\n",
    "        self.gate_conv_c = nn.utils.weight_norm(self.gate_conv_c)\n",
    "        nn.init.kaiming_normal_(self.filter_conv_c.weight)\n",
    "        nn.init.kaiming_normal_(self.gate_conv_c.weight)\n",
    "\n",
    "    def forward(self, x, c=None):\n",
    "        h_filter = self.filter_conv(x)\n",
    "        h_gate = self.gate_conv(x)\n",
    "        h_filter += self.filter_conv_c(c)\n",
    "        h_gate += self.gate_conv_c(c)\n",
    "        out = torch.tanh(h_filter) * torch.sigmoid(h_gate)\n",
    "        res = self.res_conv(out)\n",
    "        skip = self.skip_conv(out)\n",
    "        return (x + res) * sqrt(0.5), skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-athletics",
   "metadata": {},
   "source": [
    "For WaveNet it doesn't matter what it is used for: MAF or IAF - it all depends on our interpretation of the input and output variables.\n",
    "\n",
    "Below is the WaveNet architecture that you are already familiar with from the last seminar. But this time, you will need to implement not inference but forward pass - and it's very simple üòâ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(WaveNet, self). __init__()\n",
    "\n",
    "        self.front_conv = nn.Sequential(\n",
    "            CausalConv(1, params.residual_channels, params.front_kernel_size),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        for b in range(params.num_blocks):\n",
    "            for n in range(params.num_layers):\n",
    "                self.res_blocks.append(ResBlock(\n",
    "                    in_channels=params.residual_channels,\n",
    "                    out_channels=params.gate_channels,\n",
    "                    skip_channels=params.skip_channels,\n",
    "                    kernel_size=params.kernel_size,\n",
    "                    dilation=2 ** n,\n",
    "                    cin_channels=params.mel_channels))\n",
    "\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(params.skip_channels, params.skip_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(params.skip_channels, params.out_channels, kernel_size=1))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # x: input tensor with signal or noise [B, 1, T]\n",
    "        # c: local conditioning [B, C_mel, T]\n",
    "        out = 0\n",
    "        ################################################################################\n",
    "        # YOUR CODE HERE\n",
    "        ################################################################################\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that works and gives expected output size\n",
    "# full correctness we will check later, when the whole network will be assembled\n",
    "\n",
    "class Params:\n",
    "    mel_channels: int = 80\n",
    "    num_blocks: int = 4\n",
    "    num_layers: int = 6\n",
    "    out_channels: int = 3\n",
    "    front_kernel_size: int = 2\n",
    "    residual_channels: int = 64\n",
    "    gate_channels: int = 64\n",
    "    skip_channels: int = 128\n",
    "    kernel_size: int = 2\n",
    "        \n",
    "net = WaveNet(Params()).to(device).eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.FloatTensor(5, 1, 4096).normal_().to(device)\n",
    "    c = torch.FloatTensor(5, 80, 4096).zero_().to(device)\n",
    "    assert list(net(z, c).size()) == [5, 3, 4096]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-details",
   "metadata": {},
   "source": [
    "Excellent üëç! Now we are ready to get started on more complex and interesting things.\n",
    "\n",
    "Do you remember our talks about vocoders built on IAF (Parallel WaveNet or ClariNet Vocoder)? We casually said that IAF we use not just one WaveNet (predicting mu and sigma), but a stack of WaveNets. Actually, let's implement this stack, but first, a few formulas that will help you.\n",
    "\n",
    "Consider transformations of random variable $z^{(0)} \\sim \\mathcal{N}(0, I)$: \n",
    "$$z^{(0)} \\rightarrow z^{(1)} \\rightarrow \\dots \\rightarrow z^{(n)}.$$\n",
    "\n",
    "Each transformation has the form: \n",
    "$$ z^{(k)} = f^{(k)}(z^{(k-1)}) = z{(k-1)} \\cdot \\sigma^{(k)} + \\mu^{(k)},$$ \n",
    "where $\\mu^{(k)}_t = \\mu(z_{<t}^{(k-1)}; \\theta_k)$ and $\\sigma^{(k)}_t = \\sigma(z_{<t}^{(k-1)}; \\theta_k)$ ‚Äì are shifting and scaling variables modeled by a Gaussan WaveNet. \n",
    "\n",
    "It is easy to deduce that the whole transformation $f^{(k)} \\circ \\dots \\circ f^{(2)} \\circ f^{(1)}$ can be represented as $f^{(\\mathrm{total})}(z) = z \\cdot \\sigma^{(\\mathrm{total})} + \\mu^{(\\mathrm{total})}$, where\n",
    "$$\\sigma^{(\\mathrm{total})} = \\prod_{k=1}^n \\sigma^{(k)}, ~ ~ ~ \\mu^{(\\mathrm{total})} = \\sum_{k=1}^n \\mu^{(k)} \\prod_{j > k}^n \\sigma^{(j)} $$\n",
    "\n",
    "$\\mu^{(\\mathrm{total})}$ and $\\sigma^{(\\mathrm{total})}$ we will need in the future for $p(\\hat x | z) estimation$.\n",
    "\n",
    "You need to **implement** `forward` method of `WaveNetFlows` model.\n",
    "\n",
    "üìù Notes: \n",
    "1. WaveNet outputs tensor `output` of size `[B, 2, T]`, where `output[:, 0, :]` is $\\mu$ and `output[:, 1, :]` is $\\log \\sigma$. We model logarithms of $\\sigma$ insead of $\\sigma$ for stable gradients. \n",
    "2. As we model $\\mu(z_{<t}^{(k-1)}; \\theta_k)$ and $\\sigma(z_{<t}^{(k-1)}; \\theta_k)$ ‚Äì their output we have length `T - 1`. To keep constant length `T` of modelled noise variable we need to pad it on the left side (with zero).\n",
    "3. $\\mu^{(\\mathrm{total})}$ and $\\sigma^{(\\mathrm{total})}$ wil have length `T - 1`, because we do not pad distribution parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetFlows(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(WaveNetFlows, self).__init__()\n",
    "\n",
    "        self.iafs = nn.ModuleList()\n",
    "        for i in range(params.num_flows):\n",
    "            self.iafs.append(WaveNet(params))\n",
    "\n",
    "    def forward(self, z, c):\n",
    "        # z: random sample from standart distribution [B, 1, T]\n",
    "        # c: local conditioning for WaveNet [B, C_mel, T]\n",
    "        mu_tot, logs_tot = 0., 0.\n",
    "        ################################################################################\n",
    "        # YOUR CODE HERE\n",
    "        ################################################################################\n",
    "        return z, mu_tot, logs_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    num_flows: int = 4\n",
    "    mel_channels: int = 80\n",
    "    num_blocks: int = 1\n",
    "    num_layers: int = 5\n",
    "    out_channels: int = 2\n",
    "    front_kernel_size: int = 2\n",
    "    residual_channels: int = 64\n",
    "    gate_channels: int = 64\n",
    "    skip_channels: int = 64\n",
    "    kernel_size: int = 3\n",
    "        \n",
    "net = WaveNetFlows(Params()).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.FloatTensor(3, 1, 4096).normal_().to(device)\n",
    "    c = torch.FloatTensor(3, 80, 4096).zero_().to(device)\n",
    "    z_hat, mu, log_sigma = net(z, c)\n",
    "    assert list(z_hat.size()) == [3, 1, 4096]         # same length as input\n",
    "    assert list(mu.size()) == [3, 1, 4096 - 1]        # shorter by one sample\n",
    "    assert list(log_sigma.size()) == [3, 1, 4096 - 1] # shorted by one sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-challenge",
   "metadata": {},
   "source": [
    "If you are not familiar with VAE framework, please try to figure it out. For example, please familiarize with this [blog post](https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/).\n",
    "\n",
    "\n",
    "In short, VAE ‚Äì is just \"modification\" of AutoEncoder, which consists of encoder and decoder. VAE allows you to sample from data distribution $p(x)$ as $p(x|z)$ via its decoder, where $p(z)$ is simple and known, e.g. $\\mathcal{N}(0, I)$. The interesting part is that $p(x | z)$ cannot be optimized with Maximum Likelihood Estimation, because $p(x | z)$ is not tractable. \n",
    "\n",
    "But we can maximize Evidence Lower Bound (ELBO) which has a form:\n",
    "\n",
    "$$\\max_{\\phi, \\theta} \\mathbb{E}_{q_{\\phi}(z | x)} \\log p_{\\theta}(x | z) - \\mathbb{D}_{KL}(q_{\\phi}(z | x) || p(z))$$\n",
    "\n",
    "where $p_{\\theta}(x | z)$ is VAE decoder and $q_{\\phi}(z | x)$ is VAE encoder. For more details please read mentioned blog post or any other materials on this theme.\n",
    "\n",
    "In our case $q_{\\phi}(z | x)$ is represented by MAF WaveNet, and $p_{\\theta}(x | z)$ ‚Äì by IAF build with WaveNet stack. To be more precise our decoder $p_{\\theta}(x | z)$ is parametrised by the **one-step-ahead prediction** from an IAF.\n",
    "\n",
    "üßë‚Äçüíª **let's practice..**\n",
    "\n",
    "We will start from easy part: generation (or sampling). \n",
    "\n",
    "**Implement** `generate` method, which accepts mel spectrogram as conditioning tensor. Inside this method random tensor from standart distribution N(0, I) is sampled. This tensor than transformed to tensor from audio distribution via `decoder`. In the cell bellow you will see code for loading pretrained model and mel spectrogram. Listen to result ‚Äì it should sound passable, but MOS 5.0 is not expected. üòÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetVAE(nn.Module):\n",
    "    def __init__(self, encoder_params, decoder_params):\n",
    "        super(WaveNetVAE, self).__init__()\n",
    "\n",
    "        self.encoder = WaveNet(encoder_params)\n",
    "        self.decoder = WaveNetFlows(decoder_params)\n",
    "        self.log_eps = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.upsample_conv = nn.ModuleList()\n",
    "        for s in [16, 16]:\n",
    "            conv = nn.ConvTranspose2d(1, 1, (3, 2 * s), padding=(1, s // 2), stride=(1, s))\n",
    "            conv = nn.utils.weight_norm(conv)\n",
    "            nn.init.kaiming_normal_(conv.weight)\n",
    "            self.upsample_conv.append(conv)\n",
    "            self.upsample_conv.append(nn.LeakyReLU(0.4))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # x: audio signal [B, 1, T]\n",
    "        # c: mel spectrogram [B, 1, T / HOP_SIZE]\n",
    "        loss_rec = 0\n",
    "        loss_kl = 0\n",
    "        loss_frame_rec = 0\n",
    "        loss_frame_prior = 0\n",
    "        ################################################################################\n",
    "        # YOUR CODE HERE\n",
    "        ################################################################################\n",
    "        alpha = 1e-9  # for annealing during training\n",
    "        return  loss_rec + alpha * loss_kl + loss_frame_rec + loss_frame_prior\n",
    "\n",
    "    def generate(self, c):\n",
    "        # c: mel spectrogram [B, 80, L] where L - number of mel frames\n",
    "        # outputs: audio [B, 1, L * HOP_SIZE]\n",
    "        ################################################################################\n",
    "        # YOUR CODE HERE\n",
    "        ################################################################################\n",
    "        return x_sample\n",
    "\n",
    "    def upsample(self, c):\n",
    "        c = c.unsqueeze(1) # [B, 1, C, L]\n",
    "        for f in self.upsample_conv:\n",
    "            c = f(c)\n",
    "        c = c.squeeze(1) # [B, C, T], where T = L * HOP_SIZE\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved checkpoint model has following architecture parameters\n",
    "\n",
    "class ParamsMAF:\n",
    "    mel_channels: int = 80\n",
    "    num_blocks: int = 2\n",
    "    num_layers: int = 10\n",
    "    out_channels: int = 2\n",
    "    front_kernel_size: int = 32\n",
    "    residual_channels: int = 128\n",
    "    gate_channels: int = 256\n",
    "    skip_channels: int = 128\n",
    "    kernel_size: int = 2\n",
    "\n",
    "\n",
    "class ParamsIAF:\n",
    "    num_flows: int = 6\n",
    "    mel_channels: int = 80\n",
    "    num_blocks: int = 1\n",
    "    num_layers: int = 10\n",
    "    out_channels: int = 2\n",
    "    front_kernel_size: int = 32\n",
    "    residual_channels: int = 64\n",
    "    gate_channels: int = 128\n",
    "    skip_channels: int = 64\n",
    "    kernel_size: int = 3\n",
    "        \n",
    "# load checkpoint\n",
    "ckpt_path = 'data/checkpoint.pth'\n",
    "net = WaveNetVAE(ParamsMAF(), ParamsIAF()).eval().to(device)\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "net.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "# load original audio and it's mel\n",
    "x = torch.load('data/x.pth').to(device)\n",
    "c = torch.load('data/c.pth').to(device)\n",
    "\n",
    "# generate audio from \n",
    "with torch.no_grad():\n",
    "    x_prior = net.generate(c.unsqueeze(0)).squeeze()\n",
    "\n",
    "display(Audio(x_prior.cpu(), rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-registrar",
   "metadata": {},
   "source": [
    "If it sounds plausible **5 points** ü•â are already yours üéâ! And here the most interesting and difficult part comes: loss function implementation. The `forward` method will return the loss. But lets talk more precisly about our architecture and how it was trained.\n",
    "\n",
    "The encoder of our model $q_{\\phi}(z|x)$ is parametrerized by a Gaussian autoregressive WaveNet, which maps the audio $x$ into the sample length latent representation $z$. Specifically, the Gaussian WaveNet (if we talk about **real MAF**) models $x_t$ given the previous samples $x_{<t}$ with $x_t ‚àº \\mathcal{N}(\\mu(x_{<t}; \\phi), \\sigma(x_{<t}; \\phi))$, where the mean $\\mu(x_{<t}; \\phi)$ and log-scale $\\log \\sigma(x_{<t}; \\phi)$ are predicted by WaveNet, respectively.\n",
    "\n",
    "Our **encoder** posterior is constructed as\n",
    "\n",
    "$$q_{\\phi}(z | x) = \\prod_{t} q_{\\phi}(z_t | x_{\\leq t})$$\n",
    "\n",
    "where\n",
    "\n",
    "$$q_{\\phi}(z_t | x_{\\leq t}) = \\mathcal{N}(\\frac{x_t - \\mu(x_{<t}; \\phi)}{\\sigma(x_{<t}; \\phi)}, \\varepsilon)$$\n",
    "\n",
    "We apply the mean $\\mu(x_{<t}; \\phi)$ and scale $\\sigma(x_{<t})$ for \"whitening\" the posterior distribution. Also we introduce a trainable scalar $\\varepsilon > 0$ to decouple the global variation, which will make optimization process easier.\n",
    "\n",
    "Substitution of our model formulas in $\\mathbb{D}_{KL}$ formula gives:\n",
    "\n",
    "$$\\mathbb{D}_{KL}(q_{\\phi}(z | x) || p(z)) = \\sum_t \\log\\frac{1}{\\varepsilon} + \\frac{1}{2}(\\varepsilon^2 - 1 + (\\frac{x_t - \\mu(x_{<t})}{\\sigma(x_{<t})})^2)$$\n",
    "\n",
    "**Implement** calculation of `loss_kld` in `forward` method as KL divergence.\n",
    "\n",
    "---\n",
    "\n",
    "The other term in ELBO formula can be interpreted as reconstruction loss. It can be evaluated by sampling from $p_{\\theta}(x | z)$, where $z$ is from $q_{\\phi}(z | \\hat x)$, $\\hat x$ is our ground truth audio. But sampling is not differential operation! ü§î We can apply reparametrization trick!\n",
    "\n",
    "**Implement** calculation of `loss_rec` in `forward` method as recontruction loss ‚Äì which is just log likelihood of ground truth sample $x$ in predicted by IAF distribution $p_{\\theta}(x | \\hat z)$ where $\\hat z \\sim q_{\\phi}(z | \\hat x)$.\n",
    "\n",
    "--- \n",
    "\n",
    "Vocoders without MLE are still not able to train without auxilary losses. We studied many of them, but STFT-loss is our favourite!\n",
    "\n",
    "**Implement** calculation of `loss_frame_rec` which stands for MSE loss in STFT domain between original audio and its reconstruction.\n",
    "\n",
    "--- \n",
    "\n",
    "We can go even further and calculate STFT loss with random sample from $p_\\theta(x | z)$. Conditioning on mel spectrogram allows us to do so.\n",
    "\n",
    "**Implement** calculation of `loss_frame_prior` which stands for MSE loss in STFT domain between original audio and sample from prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = WaveNetVAE(ParamsMAF(), ParamsIAF()).to(device).train()\n",
    "\n",
    "x = x[:64 * 256]\n",
    "c = c[:, :64]\n",
    "\n",
    "net.zero_grad()\n",
    "loss = net.forward(x.unsqueeze(0).unsqueeze(0), c.unsqueeze(0))\n",
    "loss.backward()\n",
    "print(f\"Initial loss: {loss.item():.2f}\")\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "net.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "net.zero_grad()\n",
    "loss = net.forward(x.unsqueeze(0).unsqueeze(0), c.unsqueeze(0))\n",
    "loss.backward()\n",
    "print(f\"Optimized loss: {loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-referral",
   "metadata": {},
   "source": [
    "If you correctly implemented losses and the backward pass works smoothly, **8 more points**ü•à are yours üéâ!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-spotlight",
   "metadata": {},
   "source": [
    "For **2 additional points** ü•á please write a short essay (in russian) about your thoughts on vocoders. Try to avoid obvious statements as \"vocoder is very important part of TTS pipeline\". We are interested in insights you've got from studying vocoders. \n",
    "\n",
    "`YOUR TEXT HERE`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
