{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4667f501-a4b8-4cca-9d9f-bf1952f02679",
   "metadata": {},
   "source": [
    "# Pretraining for ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28407e-2d86-4b6f-a5de-8f6660a24c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing libs\n",
    "# !pip3 install torch torchvision torchaudio datasets transformers soundfile jiwer --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip3 install librosa --index-url https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23424d0d-66db-4a73-accb-4b5e1320a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, disable_caching, load_metric\n",
    "from transformers import Wav2Vec2ForPreTraining, Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa8ef5-4b17-4faa-a1a9-5435234f3e46",
   "metadata": {},
   "source": [
    "## Finetuning Wav2Vec2 model on CTC loss (5 points)\n",
    "\n",
    "\n",
    "In this task you have to create pipeline for finetuning pretrained multilingual Wav2Vec2 model on belarusian audio from [Fleurs](https://huggingface.co/datasets/google/fleurs) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c568-8567-497b-983c-c38c45642a9f",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8eeaa1-62f5-4ccc-8ace-74b023719835",
   "metadata": {},
   "outputs": [],
   "source": [
    "fleurs = load_dataset(\"google/fleurs\", \"be_by\", split=[\"train\", \"validation\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b35708-f6a7-4d98-b26f-da8f70d87997",
   "metadata": {},
   "outputs": [],
   "source": [
    "fleurs[0][\"transcription\"][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9aaf9-a7e8-460f-a51b-611f2bd7aaf2",
   "metadata": {},
   "source": [
    "In this task, you should:\n",
    "\n",
    "* filter all samples, where `transcription` includes digits. Hint: take care of specific belarussian symbols \"і\", \"ў\";\n",
    "* remove punctuation from `transcription`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df4169-4fce-48f0-881d-ff366ee20077",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train = # YOUR CODE HERE\n",
    "preprocessed_val = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60dd623-c031-4066-b408-4337b67056e9",
   "metadata": {},
   "source": [
    "#### Train tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bdfb6-91fd-4b3f-b1f4-acb10f122a5b",
   "metadata": {},
   "source": [
    "There you should train your own BPE tokenizer based on texts from Fleurs dataset using [HuggingFace tokenizer](https://huggingface.co/docs/tokenizers/en/training_from_memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42affbc0-7b19-4fb4-ad36-faf51a211ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import models, trainers, tokenizers, normalizers, pre_tokenizers, decoders\n",
    "\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "BOS_TOKEN = \"[BOS]\"\n",
    "EOS_TOKEN = \"[EOS]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "tokenizer = # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bee0f1-e988-477f-b608-56a5afba8f0d",
   "metadata": {},
   "source": [
    "#### Loading model and preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb4b5b-8b94-4114-ab60-600bf50c69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "   \"facebook/wav2vec2-xls-r-300m\"\n",
    ")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\", \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=tokenizer.token_to_id(PAD_TOKEN),\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d04550-d56a-4496-9a48-ea5164233fc5",
   "metadata": {},
   "source": [
    "#### Data processor and data collator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0be5db-079f-4589-9c21-80cdb9a94afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtcDataProcessor:\n",
    "    def __init__(self, tokenizer, feature_extractor):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __call__(self, row):\n",
    "        \"\"\"\n",
    "            Function applies tokenizer on row['transcription'] and applies feature extractor on audio column in row.\n",
    "            Input: dict with transcription and audio fields\n",
    "            Output: original dict includes `labels` column with tokenized sequence and `input_values` column with computed spectrogram.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42e7d4-e719-411b-bbfb-8eb9d4fb7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = CtcDataProcessor(tokenizer, feature_extractor)\n",
    "train = preprocessed_train.map(data_processor, keep_in_memory=True, remove_columns=preprocessed_train.column_names)\n",
    "val = preprocessed_val.map(data_processor, keep_in_memory=True, remove_columns=preprocessed_val.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e826f-2e25-41e0-be1f-330a2b14a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCDataCollator:\n",
    "    # HuggingFace requires pad transcript tokens with this value\n",
    "    LABELS_PAD_IDX = -100\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_tokens(tokens_batch, type, pad_value=0.0):\n",
    "        \"\"\"\n",
    "            Function collates list of tokens\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "            Function collates `input_values` and `labels` into one tensor respectively\n",
    "            Input: list with dicts, output of CTCDataProcessor\n",
    "            Output row includes `labels` column with tokenized sequence, `input_values` column with computed spectrogram and \n",
    "            `attention_mask` (0 for not-attending position, 1 for attending)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec64935-37c3-4947-a5d3-22842ca6f6ce",
   "metadata": {},
   "source": [
    "#### Inference and metrics computing\n",
    "\n",
    "There you should use simple greedy straregy for CTC output decoding. \n",
    "\n",
    "Hint: Don't forget about padding value -100 in reference.\n",
    "\n",
    "Hint: Don't forget about CTC output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c3a17-2256-4b56-8c2c-1cb0b6ae4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "class MetricsComputer:\n",
    "    def __call__(self, pred):\n",
    "        \"\"\"\n",
    "            Input: object with fields `predictions` for CTC model output and `label_ids` for tokenized reference;\n",
    "            Output: dict with key `wer` and computed wer\n",
    "        \"\"\"\n",
    "        # model prediction tensor, tensor batch_size x max_seq_len x vocab_size\n",
    "        preds_logits = pred.predictions\n",
    "        # reference, tensor batch_size x max_seq_len\n",
    "        label_ids = pred.label_ids\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        pred_str = # YOUR CODE HERE\n",
    "        label_str = # YOUR CODE HERE\n",
    "    \n",
    "        print(f\"Prediction: {pred_str[0]}\")\n",
    "        print(f\"Reference: {label_str[0]}\")\n",
    "        \n",
    "        wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "        return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fa62c-b147-4fa7-9b47-77069b7e4fb3",
   "metadata": {},
   "source": [
    "#### Overfitting on train batch\n",
    "\n",
    "In this part of task you should check pipeline correctness by overfitting on train batch. Sucessful finetuning for this task implies reasonable model transcription prediction looks like reference (even though WER metric is not 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00dd9bc-8449-4ae2-924c-660a2217cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    per_device_train_batch_size=2, # you could increase batch size\n",
    "    gradient_accumulation_steps=8, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    max_steps=3000,\n",
    "    fp16=True,\n",
    "    save_steps=50,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=# YOUR CODE HERE, \n",
    "    weight_decay=# YOUR CODE HERE,\n",
    "    warmup_steps=# YOUR CODE HERE,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad6f56-50d2-4788-8d6c-c38a279a5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=CTCDataCollator(),\n",
    "    args=training_args,\n",
    "    compute_metrics=MetricsComputer(),\n",
    "    train_dataset=train.select(range(10)),\n",
    "    eval_dataset=train.select(range(10)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362d922-6f53-4faf-820f-348674bb25a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fe4aed-e56d-4bb3-b5f6-13db02efdcdd",
   "metadata": {},
   "source": [
    "#### Bonus: real training and evaluation [10 points]\n",
    "For this bonus task you need to finetune Wav2Vec2 model and achieve 50 WER or lower accuracy on val set. This task involves memory and performance optimisations. You could start from:  \n",
    "\n",
    "* efficient training on one GPU [guide](https://huggingface.co/docs/transformers/en/perf_train_gpu_one)\n",
    "* [effective sequence sampling](https://discuss.huggingface.co/t/how-to-implement-trainers-group-by-length-in-pytorch/9232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a28c6-73be-4b2c-9343-9adedee55875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
