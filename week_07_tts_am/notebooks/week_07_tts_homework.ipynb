{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713c9f6c-02da-466e-8d9f-f51437565eb2",
   "metadata": {
    "id": "713c9f6c-02da-466e-8d9f-f51437565eb2"
   },
   "source": [
    "# 00. Homework description\n",
    "\n",
    "For this assignment, your task is to develop the [FastPitch](https://arxiv.org/abs/2006.06873) synthesis model, train it, and generate several audio samples.\n",
    "\n",
    "The training and data processing code has already been provided for you. The training will be conducted on the LJspeech dataset.\n",
    "\n",
    "The total score for the homework is **12 points**, distributed as follows:\n",
    "- 2 points for visualizing the input data\n",
    "- 8 points for writing the model code\n",
    "- 2 points for model inference\n",
    "\n",
    "The homework submission should include:\n",
    "- Completed notebook\n",
    "- Attached WER and loss graphs from TensorBoard\n",
    "- 1 audio file - the result of a regular model inference\n",
    "- 4 additional audio files - for an extra two points: you are encouraged to experiment with adjusting phoneme durations and pitch slightly and listen to the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec30da-c017-4e76-9235-bde5f55cedc4",
   "metadata": {
    "id": "97ec30da-c017-4e76-9235-bde5f55cedc4"
   },
   "source": [
    "# 01. Preparation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf0771-c271-458f-8850-56149ae94767",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "gpu_avaiable = \"1\"    # Run nvidia-smi to find free GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UPSEbEsA8z_5",
   "metadata": {
    "id": "UPSEbEsA8z_5"
   },
   "outputs": [],
   "source": [
    "path_to_repository = ...  # Path to the repository root, e.g. /home/user/speech_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b371ad4-695f-4da7-bc26-9d4936a6ec70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b371ad4-695f-4da7-bc26-9d4936a6ec70",
    "outputId": "768c54e0-6071-4fab-9ac6-e611d5415295"
   },
   "outputs": [],
   "source": [
    "# If running in colab\n",
    "\n",
    "# clone the repository:\n",
    "# git clone https://github.com/yandexdataschool/speech_course.git\n",
    "# !pip install -r speech_course/week_07_tts_am/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gibPkbI76I9y",
   "metadata": {
    "id": "gibPkbI76I9y"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We will work with [LJSpeech](https://keithito.com/LJ-Speech-Dataset/) -- a single-speaker dataset with 24 hours of speech.\n",
    "\n",
    "The data we will use contains pre-computed [MFA-alignments](https://montreal-forced-aligner.readthedocs.io/en/latest/user_guide/workflows/alignment.html) alongside with the original wavs and texts. If you are interested in the process of extracting such alignments, please refer to this [tutorial](https://colab.research.google.com/gist/NTT123/12264d15afad861cb897f7a20a01762e/mfa-ljspeech.ipynb).\n",
    "\n",
    "Download the dataset with precomputed alignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WCHQ6ev56vpI",
   "metadata": {
    "id": "WCHQ6ev56vpI"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "public_key = 'https://disk.yandex.ru/d/PpgePfWcQTAbug'\n",
    "\n",
    "final_url = base_url + urlencode(dict(public_key=public_key))\n",
    "response = requests.get(final_url)\n",
    "download_url = response.json()['href']\n",
    "response = requests.get(download_url)\n",
    "\n",
    "path_to_dataset = 'data/ljspeech'    # Choose any appropriate local path\n",
    "\n",
    "# If running in Colab:\n",
    "# path_to_dataset = '/content/ljspeech_aligned'\n",
    "\n",
    "zipfile = ZipFile(BytesIO(response.content))\n",
    "zipfile.extractall(path=path_to_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0767a-cea0-4cce-b851-dec751c6a91c",
   "metadata": {
    "id": "13f0767a-cea0-4cce-b851-dec751c6a91c"
   },
   "source": [
    "### Hi-Fi GAN checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2811e21-c864-413e-9e9a-c53a7b651dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:22:30.324480Z",
     "iopub.status.busy": "2024-03-03T13:22:30.323558Z",
     "iopub.status.idle": "2024-03-03T13:22:30.398663Z",
     "shell.execute_reply": "2024-03-03T13:22:30.397377Z",
     "shell.execute_reply.started": "2024-03-03T13:22:30.324430Z"
    },
    "id": "d2811e21-c864-413e-9e9a-c53a7b651dbf"
   },
   "source": [
    "\n",
    "Download a pretrained Hi-Fi GAN checkpoint (to generate audio from the predicted mel-spectrograms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306394e-8d09-437a-9eda-a7383749ba8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2306394e-8d09-437a-9eda-a7383749ba8f",
    "outputId": "ac9e584b-25df-41d9-e342-0befeb568b6b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/dle/hifigan__pyt_ckpt_ds-ljs22khz/versions/21.08.0_amp/zip -O hifigan_ckpt.zip\n",
    "!unzip hifigan_ckpt.zip\n",
    "!rm hifigan_ckpt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f6634-2eb0-491e-8f80-6b36122b1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In colab:\n",
    "# !wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/dle/hifigan__pyt_ckpt_ds-ljs22khz/versions/21.08.0_amp/zip -O /content/hifigan_ckpt.zip\n",
    "# !unzip /content/hifigan_ckpt.zip\n",
    "# !rm /content/hifigan_ckpt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3a5e8-094a-409b-b1a3-b17960ea3f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_hfg_ckpt = \"hifigan_gen_checkpoint_6500.pt\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d3354-5dba-4515-8e52-89767be5d3d4",
   "metadata": {
    "id": "5e6d3354-5dba-4515-8e52-89767be5d3d4"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5208b7-46cd-47bc-b43d-fc0a3d7b990d",
   "metadata": {
    "id": "0b5208b7-46cd-47bc-b43d-fc0a3d7b990d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import dataclasses\n",
    "import torch\n",
    "import subprocess as sp\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from g2p_en import G2p\n",
    "import IPython.display as Ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3562828d-37c6-41a7-9780-5269f4c20c0a",
   "metadata": {
    "id": "3562828d-37c6-41a7-9780-5269f4c20c0a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(path_to_repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed4cee2-5589-443c-81d3-3bc51e55c3eb",
   "metadata": {
    "id": "3ed4cee2-5589-443c-81d3-3bc51e55c3eb"
   },
   "source": [
    "# 02. See a data sample (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b1ba7-124e-4c0b-a13f-92d31fbfa41d",
   "metadata": {
    "id": "2d9b1ba7-124e-4c0b-a13f-92d31fbfa41d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from week_07_tts_am.fastpitch.hparams import HParamsFastpitch\n",
    "from week_07_tts_am.fastpitch.data import prepare_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FqDAP7aA_E1H",
   "metadata": {
    "id": "FqDAP7aA_E1H"
   },
   "source": [
    "The mfa alignment provides phonemes and their durations, which we will need during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_WBHdedW_WRj",
   "metadata": {
    "id": "_WBHdedW_WRj"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_dataset, 'mfa_aligned', 'LJ001-0001.json')) as f:\n",
    "  utterance = json.load(f)\n",
    "\n",
    "utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YhRhYzEMANqN",
   "metadata": {
    "id": "YhRhYzEMANqN"
   },
   "source": [
    "Phoneme `sil` here denotes pause -- a period of silence between spoken phonemes. The phonemes are from [ARPA](https://en.wikipedia.org/wiki/ARPABET) alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6c396-7b8b-4e4c-99cb-d63bb606c580",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ae6c396-7b8b-4e4c-99cb-d63bb606c580",
    "outputId": "f0086f71-e688-4715-8497-b4265fbf69d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "hparams = HParamsFastpitch()\n",
    "train_loader, val_loader = prepare_loaders(path_to_dataset, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u31B2Ofw9Zl4",
   "metadata": {
    "id": "u31B2Ofw9Zl4"
   },
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "batch = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K5Jb_3Aw9tRZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5Jb_3Aw9tRZ",
    "outputId": "d766ec17-2980-418f-e38b-1ee28817f18b"
   },
   "outputs": [],
   "source": [
    "list(dataclasses.asdict(batch).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qu_9scr2-0N4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qu_9scr2-0N4",
    "outputId": "de84900a-b7e7-4fec-e2ba-2337b1c7d320"
   },
   "outputs": [],
   "source": [
    "batch.mels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c54d1b-68e4-451d-b42b-87cf71df9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.pitches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c3a76-bb00-48d1-8e2b-b18514baf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.durations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71fd436-c5f2-4df4-ac89-63b7586613b7",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "**(1 point)** Draw a combined image showing both the mel-spectrogram and pitch for a sample from the batch. Use durations to ensure proper alignment of their shapes in the image.  \n",
    "- The visualization from the seminar might be beneficial for drawing pitch. The parameter `WINDOW_SIZE` required for determining the frequencies' resolution (df) is defined [here](https://github.com/yandexdataschool/speech_course/blob/main/week_07_tts_am/fastpitch/data.py#L108). \n",
    "   \n",
    "**(1 point)** Include phoneme labels near the time axis on the image from the previous step. (like in Figure 3 in the [paper](https://arxiv.org/pdf/2006.06873.pdf)).  \n",
    "-  Use `week_07_tts_am.fastpitch.data.SymbolsSet` to get the actual phonemes from their indices in the batch\n",
    "\n",
    "A useful code snippet:\n",
    "```py\n",
    "ax.text(\n",
    "   s, ax.get_ylim()[0] * 1.15, \n",
    "   symbol, \n",
    "   rotation=90\n",
    ")\n",
    "```\n",
    "\n",
    "`get_ylim() * 1.15` allows you to draw the phonemes lower, not directly on the spectrogram.  \n",
    "`rotation=90` will rotate them sideways, reducing overlap with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326347e-bf71-4f1a-af60-d5d2cbba195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded88b9-abb6-4b3a-bfec-1f31d8fbec5e",
   "metadata": {
    "id": "cded88b9-abb6-4b3a-bfec-1f31d8fbec5e"
   },
   "source": [
    "# 03. Implement FastPitch model (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c224ecd-1529-4c14-b7e4-d9bb2d63f2e4",
   "metadata": {},
   "source": [
    "- Please implement the FastPitch model in the cell provided below. Running this cell will overwrite the model file in the repository. \n",
    "- Run training (see next cells)\n",
    "- When submitting the homework, please include the Word Error Rate (WER) and loss curves obtained from TensorBoard as attachments.\n",
    "\n",
    "At the end of the training, if all is well, the WER should reach a value of 0.006, and the loss should reach a value of 0.69. The training will run approximately 30 minutes (3000 batches). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94899563-e8d7-44f4-8669-b67a612b21d1",
   "metadata": {
    "id": "94899563-e8d7-44f4-8669-b67a612b21d1"
   },
   "outputs": [],
   "source": [
    "%%writefile <path to repository>/week_07_tts_am/fastpitch/model.py\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "\n",
    "from week_07_tts_am.fastpitch.common.layers import TemporalPredictor\n",
    "from week_07_tts_am.fastpitch.common.utils import DeviceGetterMixin\n",
    "from week_07_tts_am.fastpitch.common.utils import regulate_len\n",
    "from week_07_tts_am.fastpitch.data import FastPitchBatch, SymbolsSet\n",
    "from week_07_tts_am.fastpitch.hparams import HParamsFastpitch\n",
    "from week_07_tts_am.fastpitch.common.transformer import FFTransformer\n",
    "\n",
    "\n",
    "class FastPitch(nn.Module, DeviceGetterMixin):\n",
    "    def __init__(self, hparams: HParamsFastpitch):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        n_symbols = len(SymbolsSet().symbols_to_id)\n",
    "\n",
    "        self.symbol_emb = nn.Embedding(n_symbols, hparams.symbols_embedding_dim)\n",
    "\n",
    "        self.encoder = FFTransformer(\n",
    "            n_layer=hparams.in_fft_n_layers,\n",
    "            n_head=hparams.in_fft_n_heads,\n",
    "            d_model=hparams.symbols_embedding_dim,\n",
    "            d_head=hparams.in_fft_d_head,\n",
    "            d_inner=4 * hparams.symbols_embedding_dim,\n",
    "            kernel_size=hparams.in_fft_conv1d_kernel_size,\n",
    "            dropout=hparams.p_in_fft_dropout,\n",
    "            dropatt=hparams.p_in_fft_dropatt,\n",
    "            dropemb=hparams.p_in_fft_dropemb\n",
    "        )\n",
    "\n",
    "        self.duration_predictor = TemporalPredictor(\n",
    "            input_size=hparams.symbols_embedding_dim,\n",
    "            filter_size=hparams.dur_predictor_filter_size,\n",
    "            kernel_size=hparams.dur_predictor_kernel_size,\n",
    "            dropout=hparams.p_dur_predictor_dropout,\n",
    "            n_layers=hparams.dur_predictor_n_layers\n",
    "        )\n",
    "\n",
    "        self.pitch_predictor = TemporalPredictor(\n",
    "            input_size=hparams.symbols_embedding_dim,\n",
    "            filter_size=hparams.pitch_predictor_filter_size,\n",
    "            kernel_size=hparams.pitch_predictor_kernel_size,\n",
    "            dropout=hparams.p_pitch_predictor_dropout,\n",
    "            n_layers=hparams.pitch_predictor_n_layers\n",
    "        )\n",
    "\n",
    "        self.pitch_emb = nn.Conv1d(1, hparams.symbols_embedding_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        self.decoder = FFTransformer(\n",
    "            n_layer=hparams.out_fft_n_layers,\n",
    "            n_head=hparams.out_fft_n_heads,\n",
    "            d_model=hparams.symbols_embedding_dim,\n",
    "            d_head=hparams.out_fft_d_head,\n",
    "            d_inner=4 * hparams.symbols_embedding_dim,\n",
    "            kernel_size=hparams.out_fft_conv1d_kernel_size,\n",
    "            dropout=hparams.p_out_fft_dropout,\n",
    "            dropatt=hparams.p_out_fft_dropatt,\n",
    "            dropemb=hparams.p_out_fft_dropemb\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Linear(hparams.symbols_embedding_dim, hparams.n_mel_channels, bias=True)\n",
    "\n",
    "    def get_encoder_out(self, batch: FastPitchBatch):\n",
    "        '''\n",
    "        Return: \n",
    "        enc_out: \n",
    "            Output of the first series of FFT blocks (before adding pitch embedding)\n",
    "            shape: (batch, len(text), symbols_embedding_dim)\n",
    "        enc_mask:\n",
    "            Boolean padding mask for the input text sequences\n",
    "            shape: (batch, len(text), 1)\n",
    "        '''\n",
    "        <YOUR CODE HERE>\n",
    "        return enc_out, enc_mask\n",
    "\n",
    "    def forward(self, batch: FastPitchBatch, use_gt_durations=True, use_gt_pitch=True, max_duration=75):\n",
    "        '''\n",
    "        Flags `use_gt_durations` and `use_gt_pitch` should be both True during training and either True or False during inference.\n",
    "\n",
    "        Use the function `regulate_len` to duplicate phonemes according to durations before passing them to the decoder.\n",
    "        \n",
    "        Return:\n",
    "        mel_out:\n",
    "            Predicted mel-spectrograms\n",
    "            shape: (batch, time, mel_bins)\n",
    "        mel_lens:\n",
    "            Number of time frames in each of the predicted spectrograms\n",
    "            shape: (batch,)\n",
    "        log_dur_pred:\n",
    "            The predicted log-durations for each phoneme (the output of the duration predictor).\n",
    "            shape: (batch, len(text))\n",
    "        dur_pred:\n",
    "            The exponent of the predicted log-durations for each phoneme. Clamped to the range (0, max_duration) for numeric stability\n",
    "            shape: (batch, len(text))\n",
    "        pitch_pred:\n",
    "            The predicted pitch for each phoneme\n",
    "            shape: (batch, len(text))\n",
    "        '''\n",
    "        <YOUR CODE HERE>\n",
    "        return mel_out, mel_lens, dur_pred, log_dur_pred, pitch_pred\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer(self, batch: FastPitchBatch, max_duration=75):\n",
    "        enc_out, dur_pred, pitch_pred = self.infer_encoder(batch, max_duration=max_duration)\n",
    "        mel_out, mel_lens = self.infer_decoder(enc_out, dur_pred)\n",
    "        return mel_out, mel_lens, dur_pred, pitch_pred\n",
    "\n",
    "    def infer_encoder(self, batch: FastPitchBatch, max_duration=75):\n",
    "        <YOUR CODE HERE>\n",
    "        return enc_out, dur_pred, pitch_pred\n",
    "\n",
    "    def infer_decoder(self, enc_out, dur_pred):\n",
    "        <YOUR CODE HERE>\n",
    "        return mel_out, mel_lens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56cee5-2819-4e64-8e02-f7b58fe54084",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Allows reloading code import without kernel restart\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44733b3d-8031-4c55-8a02-b1f7ba00eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from week_07_tts_am.fastpitch.model import FastPitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500860d-72f1-49ba-9da7-d74f836a514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FastPitch(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde33be-187f-41d9-92e0-91b105d0c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_out, enc_mask = fp.get_encoder_out(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2d810-490d-45b3-9592-e90e12d91882",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert enc_out.shape == torch.Size([hparams.batch_size, batch.texts.shape[1], hparams.symbols_embedding_dim])\n",
    "assert enc_mask.shape == torch.Size([hparams.batch_size, batch.texts.shape[1], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf709b60-8765-471f-9f48-ad04fcf34bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_out, mel_lens, dur_pred, log_dur_pred, pitch_pred = fp.forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1f96e-2dc2-4702-9711-dd196aadf3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mel_out.shape == batch.mels.transpose(2, 1).shape\n",
    "assert mel_lens.shape == batch.mel_lengths.shape\n",
    "assert dur_pred.shape == batch.texts.shape\n",
    "assert dur_pred.shape == log_dur_pred.shape\n",
    "assert pitch_pred.shape == batch.texts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba68b7-d3e8-4ea2-88dd-de4db7385ec9",
   "metadata": {
    "id": "97ba68b7-d3e8-4ea2-88dd-de4db7385ec9"
   },
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e164200-31c5-4c38-8f23-93de7ecaeaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = \"logs\"     # Choose any paths\n",
    "ckpt_dir = \"checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ae58f-2cd6-4569-babc-7abb1c6b9630",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a90a0-1356-4216-a47b-fe1d80fcefb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "783a90a0-1356-4216-a47b-fe1d80fcefb8",
    "outputId": "30504c3c-1ee6-46a5-d5d8-759bd31f4593",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sp.check_call(\n",
    "    ' '.join([\n",
    "        f'PYTHONPATH={path_to_repository} CUDA_VISIBLE_DEVICES={gpu_avaiable}',\n",
    "        f'python3 -m week_07_tts_am.fastpitch.train_fastpitch',\n",
    "        f'--logs {logs_dir}',\n",
    "        f'--ckptdir {ckpt_dir}',\n",
    "        f'--dataset {path_to_dataset}',\n",
    "        f'--hfg {path_to_hfg_ckpt}'\n",
    "    ]), shell=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810cd8b-2278-4472-b3ea-d9e6f4d8f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in colab:\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4b296-ca18-46ca-bce3-5dc3b821098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in colab:\n",
    "\n",
    "# %%shell\n",
    "\n",
    "# mkdir logs checkpoints\n",
    "\n",
    "# PYTHONPATH=speech_course python3 -m week_07_tts_am.fastpitch.train_fastpitch  \\\n",
    "# --logs logs \\\n",
    "# --ckptdir checkpoints \\\n",
    "# --dataset /content/ljspeech_aligned \\\n",
    "# --hfg /content/hifigan_gen_checkpoint_6500.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91475f-244d-431e-86c7-57ea2a9609c6",
   "metadata": {
    "id": "ee91475f-244d-431e-86c7-57ea2a9609c6"
   },
   "source": [
    "# 05. Inference (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad28736-5626-4a50-a6ba-4dcdad9bc304",
   "metadata": {
    "id": "9ad28736-5626-4a50-a6ba-4dcdad9bc304"
   },
   "outputs": [],
   "source": [
    "from week_07_tts_am.fastpitch.common.checkpointer import Checkpointer\n",
    "from week_07_tts_am.fastpitch.model import FastPitch\n",
    "from week_07_tts_am.fastpitch.data import FastPitchBatch, SymbolsSet\n",
    "from week_07_tts_am.hifigan.model import load_model as load_hfg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a451e-06de-4e9c-b45c-2461acd561cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_symbol_ids(text):\n",
    "    g2p = G2p()\n",
    "    phonemes = g2p(text)\n",
    "\n",
    "    symbols_set = SymbolsSet()\n",
    "    \n",
    "    symbols = []\n",
    "    for ph in phonemes:\n",
    "        if ph in symbols_set.symbols_to_id:\n",
    "            symbols.append(ph)\n",
    "        elif ph == ' ':\n",
    "            continue\n",
    "        else:\n",
    "            symbols.append(\"sil\")\n",
    "    \n",
    "    symbols_ids = torch.LongTensor(symbols_set.encode(symbols))\n",
    "    text_length = torch.LongTensor([symbols_ids.shape[0]])\n",
    "\n",
    "    return symbols_ids, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1541feb-f8e4-4e08-bc8a-cb66979480d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = Checkpointer(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb51ef-c337-4796-88df-50fdf928a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfg = load_hfg_model(path_to_hfg_ckpt)\n",
    "hfg = hfg.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced55d2-5b54-4957-bed1-ea8a5834ceaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ckpt_dict = checkpointer.load_last_checkpoint()\n",
    "hparams = HParamsFastpitch.create(ckpt_dict['hparams'])\n",
    "fp = FastPitch(hparams)\n",
    "fp.load_state_dict(ckpt_dict['state_dict'])\n",
    "fp = fp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23947aea-b747-4ac3-bac3-b209a23c39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9418e2e-8693-4497-b827-adcdcdec0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi! My name is Annie Wilson! Nice to meet you.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7612a-d3e7-4347-8e5c-e4c80fc277dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_ids, lengths = get_symbol_ids(text)\n",
    "\n",
    "batch = FastPitchBatch(\n",
    "    texts=symbols_ids.unsqueeze(0),\n",
    "    text_lengths=lengths\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844557b-ee3b-488f-acc5-dd43d7d17dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mels, mel_lens, *_ = fp.infer(batch)\n",
    "    mels = mels.permute(0, 2, 1)\n",
    "    audio = hfg(mels)\n",
    "\n",
    "Ipd.display(Ipd.Audio(audio.squeeze().cpu().detach().numpy(), rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b5b7e-f9fa-4c3f-9091-2d3b0f3c3110",
   "metadata": {},
   "source": [
    "## Task\n",
    "- Execute the code provided above. Then, append the generated audio to the homework results\n",
    "   - if attaching an archive, use name: `prediction.wav`\n",
    "- **(1 point)** Try increasing and decreasing the prediction speed by a factor of 2, draw spectrograms for each case\n",
    "    - if attaching an archive, use names:  `prediction_half_dur.wav`,  `prediction_double_dur.wav`\n",
    "- **(1 point)** Try shifting prediction pitch 50 Hz up and down, draw spectrograms for each case\n",
    "    - if attaching an archive, use names:  `prediction_50hz_up.wav`,  `prediction_50hz_down.wav`\n",
    "\n",
    "–êttach resulting audio files to the homework report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b3812-7561-4a74-9b7b-37e79db5d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_durations(durations: torch.Tensor, scale_factor: float):\n",
    "    <YOUR CODE HERE>\n",
    "\n",
    "\n",
    "def shift_pitch(pitch: torch.Tensor, shift: float):\n",
    "    scale = 62.51305    # Mean and variance of pitch in LJSpeech used for target pitch normalization\n",
    "    mean = 215.42230\n",
    "    <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d09e6a-d04e-4462-a748-0936b1be2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dur_pred, pitch_pred = fp.infer_encoder(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f69784-365f-495f-967a-a42876c3915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = FastPitchBatch(\n",
    "    texts=symbols_ids.unsqueeze(0),\n",
    "    text_lengths=lengths,\n",
    "    pitches=<YOUR CODE HERE>,\n",
    "    durations=<YOUR CODE HERE>\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc604d-13ed-4ff0-8574-537b3e3b7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mels, mel_lens, *_ = fp(batch, use_gt_durations=True, use_gt_pitch=True)\n",
    "    mels = mels.permute(0, 2, 1)\n",
    "    audio = hfg(mels)\n",
    "\n",
    "Ipd.display(Ipd.Audio(audio.squeeze().cpu().detach().numpy(), rate=22050))\n",
    "plt.imshow(mels.squeeze().cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74f82f-614e-47f4-9f61-c924555f2b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fastpitch",
   "language": "python",
   "name": "fastpitch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
