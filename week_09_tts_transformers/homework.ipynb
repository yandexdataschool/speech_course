{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Homework 09_codec_models [15 points]"]},{"cell_type":"markdown","metadata":{},"source":["First, let's download the required files and packages\n","\n","Uncomment the following code"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !mkdir -p codec\n","# !wget https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_09_tts_transformers/codec/codec.py -O ./codec/codec.py\n","# !wget https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_09_tts_transformers/codec/env.py -O ./codec/env.py\n","# !wget https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_09_tts_transformers/codec/model.py -O ./codec/model.py\n","# !wget https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_09_tts_transformers/codec/utils.py -O ./codec/utils.py\n","# !wget https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_09_tts_transformers/data.py\n","# !wget https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_09_tts_transformers/model.py"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !pip install deep-phonemizer librosa matplotlib numpy pyannote.audio pyloudnorm torch torchaudio tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","from urllib.parse import urlencode\n","\n","import requests\n","\n","def download_file(public_link):\n","    base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n","    final_url = base_url + urlencode(dict(public_key=public_link))\n","    response = requests.get(final_url)\n","    parse_href = response.json()['href']\n","\n","    url = parse_href\n","    start_filename = url.find('filename=')\n","    end_filename = url[start_filename:].find('&')\n","    end_name = start_filename + end_filename\n","    filename = url[start_filename:end_name][9:]\n","    download_url = requests.get(url)\n","    final_link = os.path.join(os.getcwd(), filename)\n","    with open(final_link, 'wb') as ff:\n","        ff.write(download_url.content)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### To download the file uncomment the following line\n","\n","# link_to_archive = \"https://disk.yandex.ru/d/XvDaWCWch6hWTw\"\n","# download_file(link_to_archive)\n","# !unzip lingware.zip\n","# !rm lingware.zip\n","# !mkdir -p ../data/09_tts_transformers\n","# !mv lingware ../data/09_tts_transformers"]},{"cell_type":"markdown","metadata":{},"source":["### 0. Transformer\n","\n","In this homework we will download a pretrained transformer and write inference for the model.\n","\n","First let's take a look on the required tools:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pathlib import Path\n","\n","import torch\n","import torch.nn.functional as F\n","import torchaudio\n","from IPython.display import Audio, display"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","from model import *\n","from data import *"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda:0\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Paths\n","lingware_folder = Path(\"../data/09_tts_transformers/lingware\")\n","\n","ckpt_path = lingware_folder / \"ckpt\"\n","\n","codec_model_path = lingware_folder / \"codec\" / \"ckpt\"\n","codec_config_path = lingware_folder / \"codec\" / \"config.json\"\n","\n","phonemizer_path = lingware_folder / \"phonemizer_en_us.pt\"\n","\n","dataset_url = \"dev-clean\"\n","data_path = Path(\"../data/09_tts_transformers\")\n","data_path.mkdir(exist_ok=True)"]},{"cell_type":"markdown","metadata":{},"source":["Let's download data for playing with our model. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tts_dataset = torchaudio.datasets.LIBRITTS(root=data_path, url=dataset_url, download=True)"]},{"cell_type":"markdown","metadata":{},"source":["Let's create a **phonemizer**. It will be a simple phonemizer that will use the `deep-phonemizer` library. It has 2 methods:\n","- `phonemize` - that will take a text and return a phonemized version of it, as a sequence of phonemes.\n","- `tokenize` - that will take a text, phonemize it and return a list of indices, assigned to each phoneme."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["phonemizer = Phonemizer(phonemizer_path)"]},{"cell_type":"markdown","metadata":{},"source":["Now let's create a **bioembedding model**. We will condition our model on its outputs, to mimic the speaker in synthesis.\n","\n","It has method `__call__` that takes a waveform and returns the embedding of the speaker from this waveform."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bioemb_model = BioembModel(device=device)"]},{"cell_type":"markdown","metadata":{},"source":["Now let's create a codec model. It can convert wav to codecs and back: \n","- `encode` - transforms waveform of size `[Time]` to a sequence of codecs of size `[short_time, 4]`\n","- `decode` - transforms sequence of codecs of size `[short_time, 4]` back to waveform.\n","\n","Note, that:\n","- `160` - index of end_token\n","- `161` - index of start_token\n","- `162` - index of pad_token"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["codec_model = CodecApplier(\n","    config_path=codec_config_path,\n","    ckpt_path=codec_model_path,\n","    sample_rate=16000,\n","    device=device,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Now let's assemble everything in one dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["infer_dataset = CodecsDataset(\n","    dataset=tts_dataset,\n","    phonemizer=phonemizer,\n","    bioemb_model=bioemb_model,\n","    codec_model=codec_model,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Model\n","\n","We will work with model, which mimics model from [mqtts paper](https://arxiv.org/abs/2302.04215). It consist of encoder, decoder and sub-decoder. \n","- `Encoder` consists of several Self-Attention layers and Feed-Forward layers. It takes a sequence of embeddings of phonemes and return the encoded representation of the sequence.\n","- `Decoder` consists of several Self-Attention layers and Feed-Forward layers. It uses cross-attention to watch on embeddings from encoder. It takes a sequence of codecs, for each layer creates an embeddings, concatenates them and uses as an input. Then for each codec it predicts an embedding, which is further used by sub-decoder to predict next codec.\n","- `SubDecoder` - decoder-only transformer, which gets an embedding from decoder and predicts 4 tokens. It makes 4 steps of autoregression to predict 4 tokens of the codec.\n","\n","This model was discussed on the lecture, you can refer the recording for better understanding of what is happening.\n","\n","This model was trained on LibriTTS dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ckpt = torch.load(ckpt_path, map_location=torch.device(\"cpu\"))\n","\n","model = TTSTransformer(\n","    n_phonemes=49,\n","    n_codes=163,\n","    n_codebooks=4,\n",")\n","\n","model.load_state_dict(ckpt)\n","model = model.eval().to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Inference function\n","\n","This function iterates over the input dataset `n_samples` times. For each sample predicts tokens in a teacher-forcing regime. Then decodes it with a codec_model back to waveform and plays it.\n","\n","Here we use teacher forcing, which means we use ground-truth codecs to predict the next token. This is not the best way to generate audio, but it is the simplest one for sanity check."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def infer_teacher_forcing(model, dataset, codec_model, n_samples=1, sampling_fn=lambda x: x.argmax(dim=-1)):\n","    \"\"\"\n","    model: TTSTranformer model, which has `forward` method. It gets phoneme_ids, speaker_embedding and codecs sequence and predicts logits for the next codec.\n","    dataset: Iterator over the CodecsDataset. On each iteration it shounld return tuple with (phonemes, phoneme_ids, codecs, bioemb)\n","    codec_model: Codec model, needed to decode codecs sequence back to the waveform\n","    n_samples: number of samples from the dataset which will be inferred\n","    sampling_fn: function which takes logits and returns the predicted labels. By default it returns the argmax of the logits\n","    \"\"\"\n","    device = model.parameters().__next__().device\n","\n","    for idx, (phonemes, phoneme_ids, codecs, bioemb) in zip(range(n_samples), dataset):\n","        phoneme_ids = torch.tensor([phoneme_ids], device=device)\n","        codecs = torch.tensor([codecs], device=device)\n","        bioemb = torch.tensor([bioemb], device=device)\n","\n","        phones_mask = torch.ones_like(phoneme_ids, dtype=torch.bool)\n","        codes_mask = torch.ones(codecs.shape[:2], dtype=torch.bool, device=device)\n","\n","        prediction = model(\n","            phones=phoneme_ids,\n","            phones_mask=phones_mask, # [B, l]\n","            codes=codecs, # [B, L, N]\n","            codes_mask=codes_mask, # [B, L]\n","            speaker_embs=bioemb, # [B, d]\n","        )\n","        pred_labels = sampling_fn(prediction)\n","\n","        # [:, 1:, :] is needed to remove the start tokens\n","        gt_wav = codec_model.decode(codecs[:, 1:, :], bioemb)\n","\n","        # Clamp is needed to remove the eos, bos or padding token if they emerge in the prediction\n","        pred_labels = pred_labels.clamp(min=0, max=159)\n","        synthesized_wav = codec_model.decode(pred_labels, bioemb)\n","\n","        print(f\"Phonemes: {'_'.join(phonemes)}\")\n","        print(f\"Ground truth\")\n","        display(Audio(gt_wav, rate=16000))\n","        print(f\"Synthesized\")\n","        display(Audio(synthesized_wav, rate=16000))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["infer_teacher_forcing(model, dataset=infer_dataset, codec_model=codec_model, n_samples=5)"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Sampling functions [3 points]\n","During inference our model predict logits, and we need to sample from these logits to get the next token. We will use several functions to do that.\n","- `ArgmaxSampling` - dedicated for greedy decoding, it returns the token with the highest logit (aka probability).\n","- `MultinomialSampling` - samples indices of codecs from multinomial distribution with probabilities `softmax (temperature * logits)`.\n","- `TopKSampling` - takes only tok-k logits with highest probabilities and samples from them, using multinomial sampling. \n","\n","Each function gets FloatTensor of size [\\*, logits], and returns LongTensor of size [\\*], where \\* - is the arbitrary number of dimensions.\n","\n","These function from torch can be useful:\n","- [torch.multinomial](https://pytorch.org/docs/stable/generated/torch.multinomial.html)\n","- [torch.topk](https://pytorch.org/docs/stable/generated/torch.topk.html)\n","- [torch.gather](https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: implement the following functions\n","\n","class ArgmaxSampling:\n","    def __init__(self):\n","        pass\n","\n","    def __call__(self, logits):\n","        return torch.argmax(logits, dim=-1)\n","\n","\n","class MultinomialSampling:\n","    def __init__(self, temperature=1.0):\n","        self.temperature = temperature\n","\n","    def __call__(self, logits):\n","        # Your code here\n","        raise NotImplementedError(\"TODO: assignment\")\n","        # ^^^^^^^^^^^^^^\n","\n","\n","\n","class TopKSampling:\n","    def __init__(self, k, temperature=1.0):\n","        self.k = k\n","        self.temperature = temperature\n","\n","    def __call__(self, logits):\n","        # Your code here\n","        raise NotImplementedError(\"TODO: assignment\")\n","        # ^^^^^^^^^^^^^^\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["infer_teacher_forcing(\n","    model,\n","    dataset=infer_dataset,\n","    codec_model=codec_model,\n","    n_samples=1,\n","    sampling_fn=MultinomialSampling(temperature=1.0),\n",")\n","\n","infer_teacher_forcing(\n","    model,\n","    dataset=infer_dataset,\n","    codec_model=codec_model,\n","    n_samples=1,\n","    sampling_fn=TopKSampling(k=3, temperature=1.0),\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Let's listen what we've got and how hyperparameters influence the sampling. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sampling_functions_to_test = [\n","    (MultinomialSampling, {\"temperature\": 1.}),\n","    (MultinomialSampling, {\"temperature\": 3.}),\n","    (MultinomialSampling, {\"temperature\": 0.5}),\n","    (TopKSampling, {\"k\": 7, \"temperature\": 1.}),\n","    (TopKSampling, {\"k\": 20, \"temperature\": 1.}),\n","    (TopKSampling, {\"k\": 3, \"temperature\": 1.}),\n","]\n","\n","for sampling_class, sampling_kwargs in sampling_functions_to_test:\n","    sampling_fn = sampling_class(**sampling_kwargs)\n","    print(f\"======== Sampling function: {sampling_class.__name__} with kwargs {sampling_kwargs} ========\")\n","    infer_teacher_forcing(\n","        model,\n","        dataset=infer_dataset,\n","        codec_model=codec_model,\n","        n_samples=2,\n","        sampling_fn=sampling_fn,\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["Assignment:\n","\n","What are your notions about these different sampling methods ? What is the difference between them ? What are the advantages and disadvantages of each ? Which is the preferable one ?\n"]},{"cell_type":"markdown","metadata":{},"source":["TODO"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Autoregressive inference [12 points]"]},{"cell_type":"markdown","metadata":{},"source":["Autoregressive sampling function. It creates, exactly the same, as `infer_teacher_forcing`, but uses `model.autoregressive_sampling` instead of `model.forward`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def infer_autoregressive(model, dataset, codec_model, n_samples=5, sampling_fn=lambda x: x.argmax(dim=-1)):\n","    device = model.parameters().__next__().device\n","\n","    for idx, (phonemes, phoneme_ids, codecs, bioemb) in zip(range(n_samples), dataset):\n","        phoneme_ids = torch.tensor([phoneme_ids], device=device)\n","        codecs = torch.tensor([codecs], device=device)\n","        bioemb = torch.tensor([bioemb], device=device)\n","\n","\n","        # This function is not supposed to use codecs for prediction\n","        pred_labels = model.autoregressive_sampling(\n","            phones=phoneme_ids,\n","            speaker_embs=bioemb,\n","            sampling_fn=sampling_fn,\n","        )\n","\n","        print(f\"{codecs.shape=}\")\n","        # [:, 1:, :] is needed to remove the start tokens\n","        gt_wav = codec_model.decode(codecs[:, 1:, :], bioemb)\n","\n","        # Clamp is needed to remove the eos, bos or padding token if they emerge in the prediction\n","        pred_labels = pred_labels.clamp(min=0, max=159)\n","        synthesized_wav = codec_model.decode(pred_labels, bioemb)\n","\n","        print(f\"Phonemes: {'_'.join(phonemes)}\")\n","        print(f\"Ground truth\")\n","        display(Audio(gt_wav, rate=16000))\n","        print(f\"Synthesized\")\n","        display(Audio(synthesized_wav, rate=16000))"]},{"cell_type":"markdown","metadata":{},"source":["Assignment:\n","\n","Go to the file model.py and implement SubDecoder.autoregressive_sampling and TTSTranformer.autoregressive_sampling methods.\n","\n","Notes:\n","- The model is allmost exact copy of MQTTS model from the lecture. Except that it doesn't use trick with a window in encoder-decoder attention during inference. \n","- You better not modify the `__init__` and `forward` methods of each model. Because the behaviour of the model can change.\n","- During autoregressive sampling, the model should not use the ground truth codec sequence. Instead, the model should generate the target sequence one token at a time. Starting with a vector of 4 start_tokens.\n","- You will need to figure out how the model works, so don't hesitate to print the shapes of the tensors you are working with.\n","- You will need to use SubDecoder.forward and TTSTransformer.forward methods multiple times. But do not modify them.\n","- The synthesis has two conditions that end the generation of the target sequence:\n","    - The target sequence is longer than the maximum length.\n","    - The target sequence contains at least one end token."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["infer_autoregressive(model, dataset=infer_dataset, codec_model=codec_model, sampling_fn=MultinomialSampling())"]},{"cell_type":"markdown","metadata":{},"source":["Let's say, that you have implemented those methods successefully if the model \n","geneates comprehensible speech."]},{"cell_type":"markdown","metadata":{},"source":["Now you can play with different types and hyperparameters of sampling in autoregressive synthesis."]},{"cell_type":"markdown","metadata":{},"source":["Write down:\n","- What hyperparameters you have played with ?\n","- How do they influece autoregressive synthesis ?\n","- What is their effect on audio-quality, intonation and speaker-similarity ? \n","- What are the optimal hyperparameters ?"]},{"cell_type":"markdown","metadata":{},"source":["TODO:"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
